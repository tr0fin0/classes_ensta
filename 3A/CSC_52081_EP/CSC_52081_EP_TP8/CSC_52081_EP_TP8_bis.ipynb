{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91eWH3ZiMtxH"
   },
   "source": [
    "<center>\n",
    "<h1><br/></h1>\n",
    "<h1>CSC_52081_EP lab8 bis: VAEs for MNIST digits generation</h1>\n",
    "<br>\n",
    "</center>\n",
    "\n",
    "The goal of this notebook is to present the basics of variational auto-encoders (VAEs), on some toy datasets so that training is not too long.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF7FOR0SdaJF"
   },
   "source": [
    "## Name your work\n",
    "\n",
    "Replace the values in the following dictionary `info`. Your Email must match your class email address. Your Alias will be shown on the public leaderboard (to identify yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpAx_I6Ydg8Q"
   },
   "outputs": [],
   "source": [
    "info = {\n",
    "    \"Email\": \"firstname.lastname@polytechnique.edu\",\n",
    "    \"Alias\": \"Anonymous\",  # (change this in case you want to identify yourself on the leaderboard)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86FFZePtdfNX"
   },
   "source": [
    "\n",
    "The notebook deals with variational auto-encoder (VAE). It is based on code from a <a href=\"https://dataflowr.github.io/website/\">course of Marc Lelarge</a>, which itself borrows from existing classical implementations of VAEs in PyTorch that can be found [here](https://github.com/pytorch/examples/tree/master/vae) or [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65).\n",
    "\n",
    "\n",
    "\n",
    "We will use MNIST dataset and a basic VAE architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4842,
     "status": "ok",
     "timestamp": 1710142145395,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "onVZXPp6MtxP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\n",
    "def plot_reconstruction(model, n=24):\n",
    "    x,_ = next(iter(data_loader))\n",
    "    x = x[:n,:,:,:].to(device)\n",
    "    try:\n",
    "        out, _, _, log_p = model(x.view(-1, image_size))\n",
    "    except:\n",
    "        out, _, _ = model(x.view(-1, image_size))\n",
    "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "    out_grid = torchvision.utils.make_grid(x_concat).cpu().data\n",
    "    show(out_grid)\n",
    "\n",
    "def plot_generation(model, n=24):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n, z_dim).to(device)\n",
    "        out = model.decode(z).view(-1, 1, 28, 28)\n",
    "\n",
    "    out_grid = torchvision.utils.make_grid(out).cpu()\n",
    "    show(out_grid)\n",
    "\n",
    "def plot_conditional_generation(model, n=8, z_dim=2, fix_number=None):\n",
    "    with torch.no_grad():\n",
    "        matrix = np.zeros((n,n_classes))\n",
    "        matrix[:,0] = 1\n",
    "\n",
    "        if fix_number is None:\n",
    "            final = matrix[:]\n",
    "            for i in range(1,n_classes):\n",
    "                final = np.vstack((final,np.roll(matrix,i)))\n",
    "            z = torch.randn(n*n_classes, z_dim).to(device)\n",
    "            y_onehot = torch.tensor(final).type(torch.FloatTensor).to(device)\n",
    "            concat_input = torch.cat([z, y_onehot], 1)\n",
    "            out = model.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
    "        else:\n",
    "            z = torch.randn(n, z_dim).to(device)\n",
    "            y_onehot = torch.tensor(np.roll(matrix, fix_number)).type(torch.FloatTensor).to(device)\n",
    "            out = model.decode(z,y_onehot).view(-1, 1, 28, 28)\n",
    "\n",
    "    out_grid = torchvision.utils.make_grid(out, n).cpu()\n",
    "    show(out_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710142154028,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "Ic647cGkMtxb"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a directory if not exists\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1381,
     "status": "ok",
     "timestamp": 1710142160776,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "pgwvL5SvMtxm",
    "outputId": "fb8f12d3-43e8-4309-ecf7-ef7f70e17b89"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "data_dir = 'data'\n",
    "# MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root=data_dir,\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UESJllvCMtxu"
   },
   "source": [
    "## Brief reminder on variational autoencoders\n",
    "\n",
    "Consider a latent variable model with a data variable $x\\in \\mathcal{X}$ and a latent variable $z\\in \\mathcal{Z}$, $p(z,x) = p(z)p_\\theta(x|z)$. Given the data $x_1,\\dots, x_n$, we want to train the model by maximizing the marginal log-likelihood:\n",
    "\\begin{eqnarray*}\n",
    "\\mathcal{L} = \\mathbf{E}_{p_d(x)}\\left[\\log p_\\theta(x)\\right]=\\mathbf{E}_{p_d(x)}\\left[\\log \\int_{\\mathcal{Z}}p_{\\theta}(x|z)p(z)dz\\right],\n",
    "  \\end{eqnarray*}\n",
    "  where $p_d$ denotes the empirical distribution of $X$: $p_d(x) =\\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i}(x)$.\n",
    "\n",
    " To avoid the (often) difficult computation of the integral above, the idea behind variational methods is to instead maximize a lower bound to the log-likelihood:\n",
    "  \\begin{eqnarray*}\n",
    "\\mathcal{L} \\geq L(p_\\theta(x|z),q(z|x)) =\\mathbf{E}_{p_d(x)}\\left[\\mathbf{E}_{q(z|x)}\\left[\\log p_\\theta(x|z)\\right]-\\mathrm{KL}\\left( q(z|x)||p(z)\\right)\\right].\n",
    "  \\end{eqnarray*}\n",
    "  Any choice of $q(z|x)$ gives a valid lower bound. Variational autoencoders replace the variational posterior $q(z|x)$ by an inference network $q_{\\phi}(z|x)$ that is trained together with $p_{\\theta}(x|z)$ to jointly maximize $L(p_\\theta,q_\\phi)$.\n",
    "  \n",
    "The variational posterior $q_{\\phi}(z|x)$ is also called the **encoder** and the generative model $p_{\\theta}(x|z)$, the **decoder** or generator.\n",
    "\n",
    "The first term $\\mathbf{E}_{q(z|x)}\\left[\\log p_\\theta(x|z)\\right]$ is the negative reconstruction error. Indeed under a gaussian assumption i.e. $p_{\\theta}(x|z) = \\mathcal{N}(\\mu_{\\theta}(z), I)$ the term $\\log p_\\theta(x|z)$ reduces to $\\propto \\|x-\\mu_\\theta(z)\\|^2$, which is often used in practice. The term $\\mathrm{KL}\\left( q(z|x)||p(z)\\right)$ can be seen as a regularization term, where the variational posterior $q_\\phi(z|x)$ should be matched to the prior $p(z)= \\mathcal{N}(0, I)$.\n",
    "\n",
    "Variational Autoencoders were introduced by [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114), see also [(Doersch, 2016)](https://arxiv.org/abs/1606.05908) for a tutorial.\n",
    "\n",
    "There are various examples of VAE in PyTorch available [here](https://github.com/pytorch/examples/tree/master/vae) or [here](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65). The code below is taken from this last source.\n",
    "\n",
    "![vae.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1710142398642,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "TWtQVnsAMtxw"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return torch.sigmoid(self.fc5(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2klHMA7TMtx2"
   },
   "source": [
    "Here for the loss, instead of MSE for the reconstruction loss, we take Binary Cross-Entropy. The code below is still from the PyTorch tutorial (with minor modifications to avoid warnings!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167398,
     "status": "ok",
     "timestamp": 1710142587124,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "_hs3Wnd8Mtx4",
    "outputId": "d56cab26-f1b8-4181-ec87-ee8e7f5571a3"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "\n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        # For KL divergence between Gaussians, see Appendix B in VAE paper or (Doersch, 2016):\n",
    "        # https://arxiv.org/abs/1606.05908\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, reduction='sum')\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        # Backprop and optimize\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item()/batch_size, kl_div.item()/batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinKgE7AMtx-"
   },
   "source": [
    "Let see how our network reconstructs our last batch. We display pairs of original digits and reconstructed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1710142744474,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "YLTFvi0SMtyA",
    "outputId": "7b71d311-46a6-4113-878b-b11514779d68"
   },
   "outputs": [],
   "source": [
    "plot_reconstruction(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC7nauVyMtyF"
   },
   "source": [
    "Let's see now how our network generates new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1710142753934,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "ehuLHz4NMtyH",
    "outputId": "48076334-dd2c-45fc-a69d-5073b4fa26a7"
   },
   "outputs": [],
   "source": [
    "plot_generation(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hddb3q43MtyN"
   },
   "source": [
    "Not great, but we did not train our network for long... That being said, we have no control of the generated digits. The rest of this task is to explore one solution (a conditional VAE (CVAE)) to generates zeroes, ones, twos and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OulKGKPvYpR3"
   },
   "source": [
    "# <font color=\"red\">Task</font>: implement a conditional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avGPYHDRYpR3"
   },
   "source": [
    "Implement a conditional VAE where you add a categorical variable $c\\in \\{0,\\dots 9\\}$ so that the encoder is now conditioned to two variables X and c: $q_{\\theta}(z|X,c)$ while the decoder is conditioned to the variables z ad c: $p_{\\theta}(X|z,c)$. Now, the real latent variable is distributed under $p(z|c)$.\n",
    "\n",
    "The conditional VAE was proposed by [(Sohn et al., 2015)](https://papers.nips.cc/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html) and is explained in the tutorial [(Doersch, 2016)](https://arxiv.org/abs/1606.05908).\n",
    "\n",
    "<font color=\"red\">Task</font>: Implement a class CVAE and its training by making minimal changes to the VAE code above. You can use the function plot_conditional_generation above to test your code. For instance, plot_conditional_generation(cvae, fix_number=3) should plot 8 images in the same row with independent generations of the number 3.\n",
    "\n",
    "Your are free to modify the network structure and the training parameters as long as training remains reasonable and you should try to obtain decent digits generation. Besides this, make minimal changes to the code above.\n",
    "\n",
    "Please use the code above to load the data, it checks if the data is not already present before downloading it. For ecological reasons, we will make sure to run your code in a folder that already contains the data.  \n",
    "\n",
    "Note: you will need to add \"n_classes = 10\" as this variables is used in the function \"plot_conditional_generation\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1710143641430,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "ZK-keeavYpR4"
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    # TODO: implement the class CVAE by implementing the methods\n",
    "    # __init__, encode, reparameterize, decode, forward\n",
    "    # \n",
    "\n",
    "# TODO: build model\n",
    "# \n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 835905,
     "status": "ok",
     "timestamp": 1710144517462,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "G7y4pA2XYpR4",
    "outputId": "ae4e9cdc-1d4a-4069-b335-be0477cfeb2e"
   },
   "outputs": [],
   "source": [
    "# TODO: train the model\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERJzG0e5gkSw"
   },
   "source": [
    "The following commands should then print the results generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1710144812580,
     "user": {
      "displayName": "Patrick Loiseau",
      "userId": "00996499055055304132"
     },
     "user_tz": -60
    },
    "id": "XqWwTm_QYpR5",
    "outputId": "0eafb507-223b-4ab8-f412-d2a448aba707"
   },
   "outputs": [],
   "source": [
    "#print a grid with each digit in a row (8 images of each digit per row)\n",
    "plot_conditional_generation(cvae, n=8, z_dim=4, fix_number=None)\n",
    "#print a line with just a row of digit 2 (8 images)\n",
    "plot_conditional_generation(cvae, n=8, z_dim=4, fix_number=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wcy7Y9F3TH5x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}