{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC-52081-EP Lab4: Dynamic Programming - Value Iteration and Policy Iteration\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/refs/heads/main/assets/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC-52081-EP-2025](https://moodle.polytechnique.fr/course/view.php?id=19336) Lab session #4\n",
    "\n",
    "2019-2025 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab4_rl1_dynamic_programming.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main?filepath=lab4_rl1_dynamic_programming.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/main/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab4_rl1_dynamic_programming.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/raw/main/lab4_rl1_dynamic_programming.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to introduce some classic concepts used\n",
    "in reinforcement learning like *Dynamic Programming*, *Bellman's Principle of Optimality* and *Bellman equations*.\n",
    "\n",
    "You will implement and test the two main dynamic programming algorithms (*Value Iteration* and *Policy Iteration*) in this Python notebook.\n",
    "\n",
    "You can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-52081-ep-2025-students/blob/main/lab4_rl1_dynamic_programming.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-52081-ep-2025-students/main?filepath=lab4_rl1_dynamic_programming.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-52081-ep-2025-students/raw/main/lab4_rl1_dynamic_programming.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, remember to save or download your work regularly or you may lose it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please submit your completed notebook in [Moodle : \"Lab 4 - Submission\"](https://moodle.polytechnique.fr/course/section.php?id=66532).\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "1. **File Naming:** Rename your notebook as follows: **`firstname_lastname-04.ipynb`** where `firstname` and `lastname` match your email address. *Example: `jesse_read-04.ipynb`*\n",
    "2. **Clear Output Cells:** To reduce file size (**must be under 500 KB**), clear all output cells before submitting. This includes rendered images, videos, plots, and dataframes...\n",
    "   - **JupyterLab:**\n",
    "     - Click **\"Kernel\" → \"Restart Kernel and Clear Outputs of All Cells...\"**\n",
    "     - Then go to **\"File\" → \"Save Notebook As...\"**\n",
    "   - **Google Colab:**\n",
    "     - Click **\"Edit\" → \"Clear all outputs\"**\n",
    "     - Then go to **\"File\" → \"Download\" → \"Download.ipynb\"**\n",
    "   - **VSCode:**\n",
    "     - Click **\"Clear All Outputs\"**\n",
    "     - Then **save your file**\n",
    "3. **Upload Your File:** Only **`.ipynb`** files are accepted.\n",
    "\n",
    "**Note:** Bonus parts (if any) are optional, as their name suggests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires the following Python libraries: *Gymnasium*, NumPy, Pandas and Seaborn.\n",
    "\n",
    "### If you use Google Colab\n",
    "\n",
    "Execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_requirements = [\"gymnasium\", \"numpy\", \"pandas\", \"seaborn\"]\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    for i in colab_requirements:\n",
    "        run_subprocess_command(\"pip install \" + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you use MyBinder\n",
    "\n",
    "Required libraries are already installed, you have nothing to do.\n",
    "\n",
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment\n",
    "\n",
    "Uncomment and execute the following cell to install required packages in your local environment (remove only the `#` not the `!`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install gymnasium\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: Here we assume that the reward only depends on the state: $r(\\boldsymbol{s}) \\equiv \\mathcal{R}(\\boldsymbol{s}, \\boldsymbol{a}, \\boldsymbol{s}')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Backward Induction* is a basic *Dynamic Programming* method **[BELLMAN57]**.\n",
    "Like other Dynamic Programming algorithms, it uses the *Bellman's\n",
    "Principle of Optimality* **[BELLMAN57]** for accelerating computation (compared\n",
    "to an exhaustive search). It can be applied to problems that exhibit a compatible structure, i.e., a problem that has *overlapping subproblems* or a problem having an *optimal substructure* **[BELLMAN57]**.\n",
    "Actually, this acceleration is obtained by breaking problems down into simpler subproblems in such a manner\n",
    "that redundant computations are avoided by storing results.\n",
    "When applicable, the method takes far less time than naïve methods that don't take advantage of the subproblem overlap (like depth-first search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Backward Induction* computes non-stationary policies: a new policy is computed for each time step.\n",
    "Thus the number of time steps used to solve the problem is set in advance.\n",
    "*Backward Induction* algorithms solve Sequential Decision Making problems defined with\n",
    "discrete actions and state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *value* (or *utility*) $V^*$ for each state $\\boldsymbol{s}$ at the latest time step $T$ is\n",
    "$$\n",
    "V^*_T(\\boldsymbol{s}) = r(\\boldsymbol{s})\n",
    "$$\n",
    "where $r$ is the immediate reward function.\n",
    "\n",
    "The best expected value $V^*$ for each state $\\boldsymbol{s}$ at the $t^{\\text{th}}$ time step is\n",
    "$$\n",
    "V^*_t(\\boldsymbol{s}) = r(\\boldsymbol{s}) + \\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V^*_{t+1}(\\boldsymbol{s}') \\right]  \\tag{1}\n",
    "$$\n",
    "and the $t^{\\text{th}}$ optimal action (or decision) $d^*_t(\\boldsymbol{s})$ among the set of\n",
    "possible actions $\\mathcal{A}$ is\n",
    "$$\n",
    "d^*_t(\\boldsymbol{s}) = \\arg\\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V^*_{t+1}(\\boldsymbol{s}') \\right]  \\tag{2}\n",
    "$$\n",
    "where $T$ is the transition function.\n",
    "\n",
    "The main idea is to compute the expected value of each state\n",
    "(Eq. 1) and then to use it to select the\n",
    "best action for any given state (Eq. 2).\n",
    "\n",
    "Eq. 1 cannot be solved analytically because\n",
    "the system of equations to compute $V$ contains non-linear terms (due to the\n",
    "\"max\" operator).\n",
    "As an alternative, Eq. 1\n",
    "is usually computed using Dynamic Programming method, as described in algorithm 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Algorithm 1: Backward Induction\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad$ $mdp = \\langle \\mathcal{S}, \\mathcal{A}, P, r \\rangle$, a Markov Decision Process <br>\n",
    "$\\quad$ $T$, the resolution horizon (i.e. the number of time steps) <br>\n",
    "**Local variables**: <br>\n",
    "$\\quad$ $V^*_t ~~ \\forall t \\in \\{1, ..., T\\}$, value array (expected global reward following the optimal policy for states in $\\mathcal{S}$) <br>\n",
    "<br>\n",
    "$V^*_T[\\boldsymbol{s}] \\leftarrow r(\\boldsymbol{s}) ~~ \\forall \\boldsymbol{s} \\in \\mathcal{S}$ <br>\n",
    "**for all** $t \\in \\{T-1, T-2, ..., 1\\}$ **do** <br>\n",
    "$\\quad$ **for all** $\\boldsymbol{s} \\in \\mathcal{S}$ **do** <br>\n",
    "$\\quad\\quad$ **if** $\\boldsymbol{s}$ is a final state **then** <br>\n",
    "$\\quad\\quad\\quad$ $\\displaystyle V^*_t[\\boldsymbol{s}] \\leftarrow r(\\boldsymbol{s})$ <br>\n",
    "$\\quad\\quad$ **else** <br>\n",
    "$\\quad\\quad\\quad$ $\\displaystyle V^*_t[\\boldsymbol{s}] \\leftarrow r(\\boldsymbol{s}) + \\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V^*_{t+1}[\\boldsymbol{s}'] \\right]$ <br>\n",
    "$\\quad\\quad$ **end if** <br>\n",
    "$\\quad$ **end for** <br>\n",
    "**end for** <br>\n",
    "<br>\n",
    "**return** $V^*_t ~~ \\forall t \\in \\{1, ..., T\\}$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "*Value Iteration* **[BELLMAN57]** is one of the most famous Dynamic Programming algorithm to compute the optimal policy for a Markov Decision Process (MDP).\n",
    "Similarly to Backward Induction, the\n",
    "main idea implemented by Value Iteration is to compute the best expected value of each state and then to use\n",
    "these values to select the best action from any given state.\n",
    "\n",
    "The main difference with the Backward Induction algorithm is that Value Iteration\n",
    "is used to compute stationary policies.\n",
    "Indeed, the same resulting policy is used for each time step and thus there is\n",
    "no assumption about the number of time steps to consider for the solution.\n",
    "\n",
    "The expected value $V^{\\pi}$ for each state $\\boldsymbol{s}$ when the agent follows a\n",
    "given (stationary) policy $\\pi$ is\n",
    "$$\n",
    "V^{\\pi}(\\boldsymbol{s}) = E \\left[ \\sum^{\\infty}_{t=0} \\gamma^t r(\\boldsymbol{s}_t | \\pi, \\boldsymbol{s}_0 = \\boldsymbol{s}) \\right]\n",
    "$$\n",
    "\n",
    "The optimal (stationary) policy $\\pi^*$ is defined using the best expected value $V^{\\pi^*}$ and using the principle of *Maximum Expected Utility* as follows\n",
    "$$\n",
    "\\pi^*(\\boldsymbol{s}) = \\arg\\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V^{\\pi^*}(\\boldsymbol{s}') \\right]\n",
    "$$\n",
    "\n",
    "Equation 3 is commonly called *Bellman equation*; it gives the best\n",
    "value we can expect for any given\n",
    "state (assuming the optimal policy $\\pi^*$ is\n",
    "followed). There are $|\\mathcal{S}|$ Bellman equations, one for each state.\n",
    "As for the Backward Induction method,\n",
    "this system of equations cannot be solved analytically because\n",
    "Bellman equations contain non-linear terms (due to the\n",
    "\"max\" operator).  As an alternative, Eq. 3\n",
    "can be computed iteratively using Value Iteration, a Dynamic Programming method\n",
    "described in Algorithm 2.\n",
    "\n",
    "\\begin{equation}\n",
    "    V(\\boldsymbol{s}) := V^{\\pi^*}(\\boldsymbol{s}) = \\left\\{\n",
    "    \\begin{array}{l l}\n",
    "        r(\\boldsymbol{s})                                                                                                                                 & \\quad \\text{if $\\boldsymbol{s}$ is a final state} \\\\\n",
    "        \\displaystyle r(\\boldsymbol{s}) + \\gamma \\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V(\\boldsymbol{s}') \\right]    & \\quad \\text{otherwise}\\\\\n",
    "    \\end{array} \\right.\n",
    "    \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Equation 4 -- called *Bellman update* -- is\n",
    "used in the iterative method described in Algorithm 2, to update $V$ at each iteration.\n",
    "\n",
    "\\begin{equation}\n",
    "    V_{i+1}(\\boldsymbol{s}) \\leftarrow \\left\\{\n",
    "    \\begin{array}{l l}\n",
    "        r(\\boldsymbol{s})                                                                                                                                   & \\quad \\text{if $\\boldsymbol{s}$ is a final state} \\\\\n",
    "        \\displaystyle r(\\boldsymbol{s}) + \\gamma \\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V_i(\\boldsymbol{s}') \\right]    & \\quad \\text{otherwise}\\\\\n",
    "    \\end{array} \\right.\n",
    "    \\tag{4}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Algorithm 2: Value Iteration\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad$ $mdp = \\langle \\mathcal{S}, \\mathcal{A}, P, r \\rangle$, a Markov Decision Process <br>\n",
    "$\\quad$ $\\gamma$, the discount factor <br>\n",
    "$\\quad$ $\\epsilon$, the stopping criteria: the algorithm is stopped if the largest update in an iteration is lower than $\\epsilon$ <br>\n",
    "**Local variables**: <br>\n",
    "$\\quad$ $V, V'$, old and new estimated value array (estimation of the expected global reward following the optimal policy for all states in $\\mathcal{S}$), initially zero <br>\n",
    "$\\quad$ $\\delta$, the largest change in the value array in an iteration <br>\n",
    "<br>\n",
    "**repeat** <br>\n",
    "$\\quad$ $V \\leftarrow V'$ <br>\n",
    "$\\quad$ $\\delta \\leftarrow 0$ <br>\n",
    "$\\quad$ **for all** $\\boldsymbol{s} \\in \\mathcal{S}$ **do** <br>\n",
    "$\\quad\\quad$ **if** $\\boldsymbol{s}$ is a final state **then** <br>\n",
    "$\\quad\\quad\\quad$ $\\displaystyle V'[\\boldsymbol{s}] \\leftarrow r[\\boldsymbol{s}]$ <br>\n",
    "$\\quad\\quad$ **else** <br>\n",
    "$\\quad\\quad\\quad$ $\\displaystyle V'[\\boldsymbol{s}] \\leftarrow r[\\boldsymbol{s}] + \\gamma \\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V[\\boldsymbol{s}'] \\right]$ <br>\n",
    "$\\quad\\quad$ **end if** <br>\n",
    "$\\quad\\quad$ **if** $|V'[\\boldsymbol{s}] - V[\\boldsymbol{s}]| > \\delta$ **then** <br>\n",
    "$\\quad\\quad\\quad$ $\\delta \\leftarrow |V'[\\boldsymbol{s}] - V[\\boldsymbol{s}]|$ <br>\n",
    "$\\quad\\quad$ **end if** <br>\n",
    "$\\quad$ **end for** <br>\n",
    "**until** $\\delta < \\epsilon$ <br>\n",
    "<br>\n",
    "**return** $V$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "The convergence of Value Iteration has been proved, but this convergence is asymptotic **[BELLMAN57]**.\n",
    "However, each iteration is easy and fast to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands on Gymnasium and the FrozenLake toy problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided by the Gymnasium framework.\n",
    "Gymnasium provides controllable environments (https://gymnasium.farama.org/environments/toy_text/) for research in Reinforcement Learning.\n",
    "We will use a simple toy problem to illustrate Dynamic Programming algorithms properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** read https://gymnasium.farama.org/content/basic_usage/ to discover Gymnasium and get familiar with its main concepts.\n",
    "\n",
    "In this lab, we will try to solve the FrozenLake-v1 environment (https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "Additional information is available [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this environment is *fully observable*, thus here the terms (environment) *state* and (agent) *observation* are equivalent.\n",
    "This is not always the case for example in poker, the agent doesn't know the opponent's cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the FrozenLake state space and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible states in FrozenLake are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = list(range(env.observation_space.n))\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible actions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = list(range(env.action_space.n))\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dictionary may be used to understand actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "action_labels = {0: \"Move Left\", 1: \"Move Down\", 2: \"Move Right\", 3: \"Move Up\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The next cells contain functions that can be used to display states, transitions and policies with the FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_state(\n",
    "    state_seq: Union[List[int], List[float], np.ndarray],\n",
    "    title: str = \"\",\n",
    "    figsize: Tuple[int, int] = (5, 5),\n",
    "    annot: bool = True,\n",
    "    fmt: str = \"0.1f\",\n",
    "    linewidths: float = 0.5,\n",
    "    square: bool = True,\n",
    "    cbar: bool = False,\n",
    "    cmap: str = \"Reds\",\n",
    "    heatmap: bool = True,\n",
    "    ticklabels: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Display the states in a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_seq : list of int, list of float or 1D numpy array of 16 elements\n",
    "        The sequence of states to be displayed.\n",
    "    title : str, optional\n",
    "        The title of the figure. Default is an empty string.\n",
    "    figsize : tuple of int, optional\n",
    "        The size of the figure. Default is (5,5).\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell. Default is True.\n",
    "    fmt : str, optional\n",
    "        String formatting code to use when adding annotations. Default is \"0.1f\".\n",
    "    linewidths : float, optional\n",
    "        Width of the lines that will divide each cell. Default is 0.5.\n",
    "    square : bool, optional\n",
    "        If True, set the Axes aspect to “equal” so each cell will be square-shaped. Default is True.\n",
    "    cbar : bool, optional\n",
    "        If True, draw a colorbar. Default is False.\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space. Default is \"Reds\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate the size of the square array\n",
    "    size = int(math.sqrt(len(state_seq)))\n",
    "\n",
    "    # Convert the state sequence to a numpy array\n",
    "    state_array = np.array(state_seq)\n",
    "\n",
    "    # Reshape the array into a square\n",
    "    state_array = state_array.reshape(size, size)\n",
    "\n",
    "    # Create a new figure and axes with the specified size\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create a heatmap of the state array\n",
    "    sns.heatmap(\n",
    "        state_array,\n",
    "        annot=annot,\n",
    "        fmt=fmt,\n",
    "        linewidths=linewidths,\n",
    "        square=square,\n",
    "        cbar=cbar,\n",
    "        cmap=cmap if heatmap else \"crest\",\n",
    "        vmin=None if heatmap else 0.0,\n",
    "        vmax=None if heatmap else 0.0,\n",
    "        xticklabels=ticklabels,\n",
    "        yticklabels=ticklabels,\n",
    "    )\n",
    "\n",
    "    # Set the title of the heatmap\n",
    "    plt.title(title)\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_transition(state: int, action: int) -> None:\n",
    "    \"\"\"\n",
    "    Display the transition probabilities for a given action in a given state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The state for which to display the transition probabilities.\n",
    "    action : int\n",
    "        The action for which to display the transition probabilities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Use the display_state function to create a heatmap of the transition probabilities\n",
    "    # for the given action in the given state. The transition probabilities are stored\n",
    "    # in the global transition_array variable, and the labels for the actions are stored\n",
    "    # in the global action_labels variable.\n",
    "    display_state(\n",
    "        transition_array[state, action],\n",
    "        title=f\"Transition probabilities for action {action} ({action_labels[action]}) in state {state}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Display the policy as a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : list of int\n",
    "        The policy to be displayed. Each integer represents an action to be taken in a state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a list of actions with their corresponding labels\n",
    "    actions_src = [\n",
    "        f\"{action}={action_labels[action].replace('Move ', '')}\" for action in actions\n",
    "    ]\n",
    "\n",
    "    # Create a title for the heatmap using the actions and their labels\n",
    "    title = f\"Policy ({', '.join(actions_src)})\"\n",
    "\n",
    "    # Use the display_state function to create a heatmap of the policy\n",
    "    # The fmt parameter is set to \"d\" to display integers, cbar is set to False to not display a colorbar,\n",
    "    # and cmap is set to \"Reds\" to use the Reds color map\n",
    "    display_state(policy, title=title, fmt=\"d\", cbar=False, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the `is_final_array`, `reward_array` and `transition_array`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Dynamic Programming algorithms, we need the transition probability (or transition function) and the reward function, both defined in `env.unwrapped.P`.\n",
    "\n",
    "`env.unwrapped.P[S][A]` gives the list of reachable states from state S executing action A.\n",
    "\n",
    "These reachable states are coded in a tuple defined like this: `(probability, next state, reward, is_final_state)`.\n",
    "\n",
    "You will not need to use `env.unwrapped.P` to solve exercises.\n",
    "In the following cell, `is_final_array`, `reward_array` and `transition_array` are defined for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "is_final_array = np.full(shape=len(states), fill_value=np.nan, dtype=bool)\n",
    "reward_array = np.full(shape=len(states), fill_value=-np.inf)                # -np.inf = negative infinity\n",
    "transition_array = np.zeros(shape=(len(states), len(actions), len(states)))\n",
    "\n",
    "for state in states:\n",
    "    for action in actions:\n",
    "        for next_state_tuple in env.unwrapped.P[state][action]:              # env.unwrapped.P[state][action] contains the next states list (a list of tuples)\n",
    "            transition_probability, next_state, next_state_reward, next_state_is_final = next_state_tuple\n",
    "\n",
    "            is_final_array[next_state] = next_state_is_final\n",
    "            reward_array[next_state] = max(reward_array[next_state], next_state_reward)   # workaround: when we already are in state 15, reward is 0 if we stay in state 15 (in practice this never append as the simulation stop when we arrive in state 15 as any other terminal state)\n",
    "            transition_array[state, action, next_state] += transition_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the state corresponding to square of the FrozenLake grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_state(states, fmt=\"d\", title=\"States ID\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the reward obtained in each square of the FrozenLake reachable_statesgrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_state(reward_array, title=\"Rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows whether a square is a final state or not (i.e. whether it ends the simulation or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_state(is_final_array, fmt=\"d\", title=\"Final states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells show how to display transitions with the provided `display_transition` function. Figures displayed in squares are the probability to reach these squares from the given (`state`, `action`) pair. Colored squares are the states that may be reached from this pair (a non-zero probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_transition(state=0, action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_transition(state=6, action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "display_transition(state=6, action=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_transition(state=6, action=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_transition(state=6, action=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement the Value Iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the FrozenLake-v1 problem with Dynamic Programming, we will first use the Value Iteration algorithm described in Algorithm 2.\n",
    "\n",
    "Notice that the FrozenLake-v1 environment is non-deterministic.\n",
    "To implement Value Iteration, you will need the transition probability (or the transition function) defined in `transition_array`.\n",
    "- Use `transition_array[S, A]` to get the probability of reaching each state from state `S` executing action `A`.\n",
    "- Use `transition_array[S, A, S']` to get the probability of reaching state `S'` from state `S` executing action `A`.\n",
    "\n",
    "You will also need the previously defined `is_final_array` matrix.\n",
    "- Use `is_final_array[S]` to know whether `S` is a final state (`True`) or not (`False`).\n",
    "\n",
    "Finally, you will need the previously defined `reward_array` matrix.\n",
    "- Use `reward_array[S]` to get the reward obtained by the agent each time it reaches state `S`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In the following cell, we define `expected_value` and `expected_values` functions for convenience.\n",
    "The first one returns the expected reward\n",
    "$$\\sum P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V(\\boldsymbol{s}')$$\n",
    "for a given pair $(\\boldsymbol{s}, \\boldsymbol{a})$ and a given V-table (value function) $V$.\n",
    "The second one computes the expected reward for all the actions in $\\boldsymbol{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_value(state: int, action: int, v_array: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the expected value for a given state-action pair.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The state from which the action is taken.\n",
    "    action : int\n",
    "        The action that is taken.\n",
    "    v_array : np.ndarray\n",
    "        The value function represented as a numpy array.\n",
    "        `v_array` is a 1D numpy array of 16 floats (one value per state).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The expected value for the given state-action pair.\n",
    "    \"\"\"\n",
    "    # The transition_array is a global variable that contains the transition probabilities for each state-action pair.\n",
    "    # The expected value for a state-action pair is computed as the sum of the product of the transition probabilities\n",
    "    # and the values of the states. This is represented by the expression (transition_array[state, action] * v_array).sum().\n",
    "    return (transition_array[state, action] * v_array).sum()  # compute sum(P(s'|a,s).V(s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_values(state: int, v_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the expected values for all actions from a given state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The state from which the actions are taken.\n",
    "    v_array : np.ndarray\n",
    "        The value function represented as a numpy array.\n",
    "        `v_array` is a 1D numpy array of 16 floats (one value per state).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A numpy array of expected values for all actions from the given state,\n",
    "        i.e. a 1D numpy array of 4 floats (one value per action).\n",
    "    \"\"\"\n",
    "    # The transition_array is a global variable that contains the transition probabilities for each state-action pair.\n",
    "    # The expected values for all actions from a given state are computed as the sum of the product of the transition probabilities\n",
    "    # and the values of the states. This is represented by the expression (transition_array[state] * v_array).sum(axis=1).\n",
    "    # The axis=1 parameter means that the sum is computed over the second axis (i.e., for each action).\n",
    "\n",
    "    # print(f\"\\t{transition_array[state]} {v_array}\")\n",
    "    return (transition_array[state] * v_array).sum(axis=1)   # compute sum(P(s'|a,s).V(s')) for all the actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement the Value Iteration algorithm (compute the *value function* `v_array`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: here we use the `state_display` function to show the evolution of the value function `v_array` over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = False\n",
    "\n",
    "def value_iteration(\n",
    "    gamma: float = 0.95,\n",
    "    epsilon: float = 0.001,\n",
    "    display: bool = False,\n",
    ") -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Perform the value iteration algorithm for a given set of states and actions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float, optional\n",
    "        The discount factor, by default 0.95\n",
    "    epsilon : float, optional\n",
    "        The convergence threshold, by default 0.001\n",
    "    display : bool, optional\n",
    "        A flag indicating whether to display the value function at each iteration, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The final value function after the value iteration algorithm has converged,\n",
    "        i.e. a 1D numpy array of 16 floats (one value per state).\n",
    "    List[np.ndarray]\n",
    "        The history of the value function at each iteration (a list of 1D numpy arrays of 16 floats).\n",
    "    List[float]\n",
    "        The history of the maximum change in the value function at each iteration (the \"Bellman residual\" $\\delta$).\n",
    "    \"\"\"\n",
    "    # Initialize the history of the value function and the maximum change in the value function\n",
    "    value_function_history = []\n",
    "    delta_history = []\n",
    "\n",
    "    # Initialize the value function with zeros\n",
    "    v_array = np.zeros(len(states))\n",
    "    stop = False\n",
    "\n",
    "    # Continue the loop until the value function converges\n",
    "    while not stop:\n",
    "        # Display the value function if the display flag is set\n",
    "        if display:\n",
    "            display_state(v_array, title=f\"Value function (iteration { len(value_function_history) })\", cbar=False)\n",
    "        else:\n",
    "            print(\".\", end=\"\")\n",
    "\n",
    "        # Append the current value function to the history\n",
    "        value_function_history.append(v_array.copy())\n",
    "\n",
    "        # Initialize the maximum change in the value function to zero\n",
    "        delta = 0.0\n",
    "\n",
    "        new_v_array = np.zeros(len(states))\n",
    "\n",
    "        # TODO...\n",
    "        for state in states:\n",
    "            if is_final_array[state]:\n",
    "                new_v_array[state] = reward_array[state]\n",
    "            else:\n",
    "                new_v_array[state] = reward_array[state] + gamma * np.max(expected_values(state, v_array))\n",
    "\n",
    "            delta = max(delta, abs(new_v_array[state] - v_array[state]))\n",
    "\n",
    "        delta_history.append(delta)\n",
    "        v_array = new_v_array\n",
    "\n",
    "        if delta < epsilon:\n",
    "            stop = True\n",
    "\n",
    "    # Return the final value function, the history of the value function, and the history of the maximum change in the value function\n",
    "    return v_array, value_function_history, delta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_array, value_function_history, delta_history = value_iteration(display=True)\n",
    "e1q1_answer = v_array[0].item()\n",
    "display_state(v_array, title=f\"Value function (iteration { len(value_function_history) })\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please copy and paste the output of the following cell into the first question of the *Lab 4 - Evaluation* in Moodle:  \n",
    "*\"What is the expected value of the initial state in the FrozenLake environment (using the default discount factor, γ)?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{e1q1_answer:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the evolution of the value function over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_hist = pd.DataFrame(value_function_history)\n",
    "df_v_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of `v_array` (the estimated value of each state) over iterations (one curve per state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_hist.plot(figsize=(14, 8))\n",
    "plt.title(\"V(s) w.r.t iteration\")\n",
    "plt.ylabel(\"V(s)\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of `delta` over iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.plot(delta_history)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(r\"$\\max~\\delta$ w.r.t iteration\")\n",
    "plt.ylabel(r\"$\\max~\\delta$\")\n",
    "plt.xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Task 2: Define the greedy policy (Maximum Expected Utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state: int, v_array: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Compute the policy that maximizes the expected value for a given state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The state for which the policy is computed.\n",
    "    v_array : np.ndarray\n",
    "        The value function represented as a numpy array.\n",
    "        `v_array` is a 1D numpy array of 16 floats (one value per state).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The policy that maximizes the expected value for the given state.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return np.argmax(expected_values(state, v_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the opimized policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the `greedy_policy` on each state gives us the policy matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = [greedy_policy(state, v_array) for state in states]\n",
    "e1q2_answer = policy[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gives us a graphical representation of the optimal policy we have computed. The figure in each square is the optimal action to execute in the corresponding state (0 = \"move left\", 1 = \"move down\", 2 = \"move right\", 3 = \"move up\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please copy and paste the output of the following cell into the second question of the *Lab 4 - Evaluation* in Moodle:  \n",
    "*\"What is the optimal action for the 9th state in the FrozenLake environment (using the default discount factor, γ)?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e1q2_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Value Iteration with Gymnasium (single trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have computed the value function `v_array` for one *episode*.\n",
    "The environment is stochastic, thus if we apply the computed policy several times on the environment, we may have different results.\n",
    "To measure the performance of our value function `v_array`, we should assess it several times and count the number of successful trials.\n",
    "Gymnasium considers an agent to successfully solve the FrozenLake problem if it reaches 76% success rate over the last 100 trials (or \"episodes\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_reward_list: List[float] = []\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    # t = 0\n",
    "\n",
    "    while not done:\n",
    "        action = greedy_policy(state, v_array)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        # t += 1\n",
    "\n",
    "    vi_reward_list.append(float(reward))\n",
    "    # print(\"Episode finished after {} timesteps ; reward = {}\".format(t, reward))\n",
    "\n",
    "print(sum(vi_reward_list) / NUM_EPISODES)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you think the discount factor $\\gamma$ is for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discount rate determines the present value of future rewards.\n",
    "If $\\gamma = 0$, the agent is \"myopic\" in being concerned only with maximizing the immediate rewards.\n",
    "As $\\gamma$ approaches 1, the return objective takes future rewards into account more strongly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Value Iteration for different value of $\\gamma$ with confidence interval (bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "vi_gamma_reward_list: List[Dict[str, float]] = []\n",
    "\n",
    "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.84, 0.9, 0.95, 0.99):\n",
    "    v_array, value_function_history, delta_history = value_iteration(gamma=gamma)\n",
    "\n",
    "    for episode_index in range(NUM_EPISODES):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = greedy_policy(state, v_array)\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        vi_gamma_reward_list.append({\"gamma\": gamma, \"reward\": float(reward)})\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vi_gamma_reward_list)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean reward (with its 95% confidence interval)\n",
    "\n",
    "sns.relplot(x=\"gamma\", y=\"reward\", kind=\"line\", data=df, height=6, aspect=1.5)\n",
    "plt.axhline(0.76, color=\"red\", linestyle=\":\", label=\"76% success threshold\");   # 76% success threshold\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Value Iteration optimal policy with respect to $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99):\n",
    "    print()\n",
    "    print(\"=\" * 10, \"GAMMA = \", gamma, \"=\" * 10)\n",
    "    print()\n",
    "\n",
    "    v_array, value_function_history, delta_history = value_iteration(gamma=gamma)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    policy = [greedy_policy(state, v_array) for state in states]\n",
    "    display_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement the Policy Iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Policy Iteration* **[HOWARD60]** is another popular Dynamic Programming algorithm to\n",
    "compute MDP's optimal policy. In practice, it is often faster than Value Iteration.\n",
    "\n",
    "The Policy Iteration algorithm alternates the following two steps, starting with an initial policy $\\pi_0$:\n",
    "1. **Policy Evaluation**: given a policy $\\pi_i$, compute $V^{\\pi_i}(\\boldsymbol{s}) ~ \\forall \\boldsymbol{s} \\in \\mathcal{S}$, the expected value of each state when $\\pi_i$ is followed.\n",
    "2. **Policy Improvement**: compute a new policy $\\pi_{i+1}$, using one-step look-ahead based on $V^{\\pi_i}$ and using the principle of *Maximum Expected Utility* as follows\n",
    "$$\n",
    "\\pi_{i+1}(\\boldsymbol{s}) = \\arg \\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\sum_{\\boldsymbol{s}'  \\in \\mathcal{S}} P(\\boldsymbol{s}'|\\boldsymbol{s},\\boldsymbol{a}) \\left( r(\\boldsymbol{s},\\boldsymbol{a},\\boldsymbol{s}') + \\gamma V^{\\pi_{i}}(\\boldsymbol{s}') \\right)\n",
    "$$\n",
    "\n",
    "In the following exercise we will assume that the reward only depends on the state: $r(\\boldsymbol{s}) \\equiv \\mathcal{R}(\\boldsymbol{s}, \\boldsymbol{a}, \\boldsymbol{s}')$.\n",
    "Thus the *Policy Improvement* can be rewritten as follow:\n",
    "\n",
    "$$\n",
    "\\pi_{i+1}(\\boldsymbol{s}) = \\arg\\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\boldsymbol{a}) V^{\\pi_i}(\\boldsymbol{s}')\n",
    "$$\n",
    "\n",
    "($r$ and $\\gamma$ can disappear as they have no influence on the $\\arg\\max_{\\boldsymbol{a}}$ result).\n",
    "\n",
    "Algorithm 3 describes the two-step procedure.\n",
    "The algorithm terminates when the *Policy Improvement* step yields no change in the utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Algorithm 3: Policy Iteration\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad$ $MDP = \\langle \\mathcal{S}, \\mathcal{A}, P, r \\rangle$, a Markov Decision Process<br>\n",
    "**Local variables**: <br>\n",
    "$\\quad$ $V$, vector of utilities for states in $\\mathcal{S}$, initially zero <br>\n",
    "$\\quad$ $\\pi$, a policy vector indexed by state, initially random <br>\n",
    "<br>\n",
    "**repeat** <br>\n",
    "$\\quad$ $V \\leftarrow \\text{POLICY-EVALUATION}(\\pi, V, \\text{MDP})$ <br>\n",
    "$\\quad$ unchanged $\\leftarrow$ true <br>\n",
    "$\\quad$ **for all** state $\\boldsymbol{s} \\in \\mathcal{S}$ **do** <br>\n",
    "$\\quad\\quad$ **if** $\\displaystyle \\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}'|\\boldsymbol{s},\\boldsymbol{a}) V[\\boldsymbol{s}'] \\right] > \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\pi_{i}(\\boldsymbol{s})) V[\\boldsymbol{s}']$ **then** <br>\n",
    "$\\quad\\quad\\quad$ $\\displaystyle \\pi[\\boldsymbol{s}] \\leftarrow \\arg\\max_{\\boldsymbol{a} \\in \\mathcal{A}} \\left[ \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}'|\\boldsymbol{s},\\boldsymbol{a}) V[\\boldsymbol{s}'] \\right]$ <br>\n",
    "$\\quad\\quad\\quad$ unchanged $\\leftarrow$ false <br>\n",
    "$\\quad\\quad$ **end if** <br>\n",
    "$\\quad$ **end for** <br>\n",
    "**until** unchanged <br>\n",
    "<br>\n",
    "**return** $\\pi$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the POLICY-EVALUATION routine is much simpler than solving the standard\n",
    "Bellman equations (which is what Value Iteration does).  Indeed, the action in each\n",
    "state is fixed by the policy, thus the \"max\" operator disappears and Bellman\n",
    "equations become linear.\n",
    "As a result, $V^{\\pi_i}$ can be computed by solving the linear system of these\n",
    "*simplified Bellman equations* (Eq. 5) for\n",
    "each state.\n",
    "\n",
    "\\begin{equation}\n",
    "    V^{\\pi_i}(\\boldsymbol{s}) = \\left\\{\n",
    "    \\begin{array}{l l}\n",
    "        r(\\boldsymbol{s})               & \\quad \\text{if $\\boldsymbol{s}$ is a final state} \\\\\n",
    "        \\displaystyle r(\\boldsymbol{s}) + \\gamma \\sum_{\\boldsymbol{s}' \\in \\mathcal{S}} P(\\boldsymbol{s}' | \\boldsymbol{s}, \\pi_{i}(\\boldsymbol{s})) ~ V^{\\pi_{i}}(\\boldsymbol{s}')     & \\quad \\text{otherwise}\\\\\n",
    "    \\end{array} \\right.\n",
    "    \\tag{5}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "As the number of states and policies is finite, and as the policy is improved\n",
    "at each iteration, Policy Iteration converges in a finite number of iterations (often\n",
    "small in practice).\n",
    "However, within each iteration, solving the\n",
    "POLICY-EVALUATION routine may cost a lot (its complexity is $O(|\\mathcal{S}|^3)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach alternative to Value Iteration (Exercise 1) is Policy Iteration (described in Algorithm 3).\n",
    "\n",
    "**Task:** implement Iterative Policy Iteration (for the same environment). Note that as part of this task you should also implement iterative policy evaluation. Compare the policies obtained by both approaches (they should be the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Task 1: Define the (exact) Policy Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy: np.ndarray, gamma: float, theta: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate a policy by solving a system of linear equations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : np.ndarray\n",
    "        The policy to evaluate, represented as a numpy array.\n",
    "        `policy` is a 1D numpy array of 16 integers (one action per state).\n",
    "    gamma : float\n",
    "        The discount factor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The value function for the given policy,\n",
    "        i.e. a 1D numpy array of 16 floats (one value per state).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO...\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = 0\n",
    "            for prob, next_state, reward, _ in env.unwrapped.P[s][policy[s]]:\n",
    "                v += prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Define the Policy Improvement function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "    gamma: float,\n",
    "    initial_policy: Optional[np.ndarray] = None,\n",
    "    policy_evaluation_function: Callable[[np.ndarray, float], np.ndarray] = policy_evaluation\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform the policy iteration algorithm for a given set of states and actions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float\n",
    "        The discount factor.\n",
    "    initial_policy : np.ndarray, optional\n",
    "        The initial policy, by default None. If None, a random initial policy is used.\n",
    "        If provided, `initial_policy` is a 1D numpy array of 16 integers (one action per state).\n",
    "    policy_evaluation_function : Callable[[np.ndarray, float], np.ndarray], optional\n",
    "        The function to use for policy evaluation, by default `policy_evaluation`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The optimal policy after the policy iteration algorithm has converged,\n",
    "        i.e. a 1D numpy array of 16 integers (one action per state).\n",
    "    \"\"\"\n",
    "    # TODO...\n",
    "    def policy_improvement(V, gamma):\n",
    "        policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "        for s in range(env.observation_space.n):\n",
    "            action_values = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for prob, next_state, reward, _ in env.unwrapped.P[s][a]:\n",
    "                    action_values[a] += prob * (reward + gamma * V[next_state])\n",
    "            policy[s] = np.argmax(action_values)\n",
    "\n",
    "        return policy\n",
    "\n",
    "    policy = np.random.choice(env.action_space.n, size=env.observation_space.n)\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, gamma)\n",
    "        new_policy = policy_improvement(V, gamma)\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "policy = policy_iteration(gamma=gamma, policy_evaluation_function=policy_evaluation)\n",
    "\n",
    "display_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Policy Iteration with Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_reward_list: List[float] = []\n",
    "\n",
    "NUM_EPISODES = 10000\n",
    "\n",
    "for episode_index in range(NUM_EPISODES):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    # t = 0\n",
    "\n",
    "    while not done:\n",
    "        action = policy[state]\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        # t += 1\n",
    "\n",
    "    pi_reward_list.append(float(reward))\n",
    "    # print(\"Episode finished after {} timesteps ; reward = {}\".format(t, reward))\n",
    "\n",
    "e2q3_answer = sum(pi_reward_list) / NUM_EPISODES\n",
    "print(e2q3_answer)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please copy and paste the output of the following cell into the first question of the *Lab 4 - Evaluation* in Moodle:  \n",
    "*\"What is the success rate of Policy Iteration on the FrozenLake environment (using the default discount factor, γ)?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{e2q3_answer:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Policy Iteration for different $\\gamma$ with confidence interval (bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "pi_gamma_reward_list: List[Dict[str, float]] = []\n",
    "\n",
    "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.99):\n",
    "    print(\"gamma:\", gamma)\n",
    "    policy = policy_iteration(gamma=gamma)\n",
    "\n",
    "    for episode_index in range(NUM_EPISODES):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        pi_gamma_reward_list.append({\"gamma\": gamma, \"reward\": float(reward)})\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pi_gamma_reward_list)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mean reward (with its 95% confidence interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"gamma\", y=\"reward\", kind=\"line\", data=df, height=6, aspect=1.5)\n",
    "plt.axhline(0.76, color=\"red\", linestyle=\":\", label=\"76% success threshold\");   # 76% success threshold\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[BELLMAN57]** Richard Ernest Bellman. *Dynamic Programming*. Princeton University Press, Princeton,\n",
    "New Jersey, USA, 1957. [[2010 reissue]](https://press.princeton.edu/books/paperback/9780691146683/dynamic-programming?_gl=1*1vmwsa4*_up*MQ..*_ga*OTYzMTUzOTU0LjE3MzgwMDI1NjY.*_ga_N1W9JWKLY3*MTczODAwMjU1OC4xLjEuMTczODAwMjY5NS4wLjAuNzE0NjQxNTkx) [[Nearby libraries]](https://focus.universite-paris-saclay.fr/primo-explore/search?query=any,contains,9780691146683&tab=default_tab&search_scope=default_scope&sortby=date&vid=33PUP_VU1&facet=frbrgroupid,include,917809168&lang=en_US&offset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[HOWARD60]** R.A. Howard. Dynamic Programming and Markov Processes. MIT Press, Cambridge,\n",
    "Massachusetts, 1960."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b82aa116ac616c8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Going further\n",
    "\n",
    "In this lab we have introduced Reinforcement Learning in a very specific case where the *agent* (the algorithm) has a perfect knowledge of the environment (transition and reward functions).\n",
    "\n",
    "This is convenient to introduce basic concepts but we cannot expect this assumption to be true in many practical problems.\n",
    "A lot of sophisticated algorithms have been developed recently and most of them have been implemented in [Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/) library and can be used in [Gymnasium](https://gymnasium.farama.org/) benchmark library.\n",
    "\n",
    "Also, for those who want to go further, one of the best book in reinforcement learning is freely available on the web: http://incompleteideas.net/book/RLbook2018.pdf"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
