{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf88504",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# CSC_52081_EP - Lab 02\n",
    "\n",
    "### Main Objectives \n",
    "\n",
    "As per last week, we study an environment in the context of *perception* (observation), *knowledge* (representation), *reasoning* (inference), and *acting* (decision-making). However, today we will additionally study *learning*, and bear in mind aspects of *scalability*; notably the fact that it is usually unreasonable to fully explore and evaluation all possible decisions; and it is typically unreasonable also to assume that we have full and accurate specifification of the environment dynamics (characteristics of the scenaario we are working on). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a204f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e184c",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Inference as Search \n",
    "\n",
    "For inference in the context of multi-output and structured prediction, there is often an intractable number of possibilities; too many to enumerate in a brute force fashion. This is often the case in multi-label classification, and it can be the case in scenarios such as we looked at in Lab 01, where we have a probability tree. In that case, the probability tree was easy to enumerate, but this time we have modified the `Environment` class in `environment.py` to produce a more dense tree (namely, by allowing false-positive sound emissions); and producing longer trajectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment\n",
    "\n",
    "# Instantiate the environment\n",
    "G = np.array([[1,3,0,2,4,1],\n",
    "              [2,1,0,3,0,3],\n",
    "              [4,0,3,0,2,0],\n",
    "              [3,1,2,3,0,4],\n",
    "              [2,0,0,0,1,1]])\n",
    "# The fps flag allows for false positives (make sure you have the recent version of envioronment.py)\n",
    "env = Environment(G,fps=True)\n",
    "# Generate a single step, it should be possible to see a false positive appear\n",
    "s, o = env.step()\n",
    "ooo = np.array([o])\n",
    "sss = np.array([s]).reshape(1,-1)\n",
    "fig, ax = env.render(sss, ooo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31869018",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "So, now let's take a fresh look at Lab 01, specifically the function `P_traj`. Go back to that function and make use of the `M` parameter to allow for abritrarily efficient inference; in particular via **Monte Carlo Search**. Essentially, you should sample `M` trajectories as an approximation:  \n",
    "$$\n",
    "    \\{s^{(m)}_1,\\ldots,s^{(m)}_T, p^{(m)}\\}_{m=1}^M \\approx P(S_1,\\ldots,S_T \\mid \\vec{x}_1,\\ldots,\\vec{x}_T)\n",
    "$$\n",
    "You can use what is known as **ancestral sampling**, since you have access to the environment, sample $s_1 \\sim P(s_1 | \\vec{x}_1)$, then $s_2 \\sim P(s_2 | s_1)$ and so on (similarly to how you probably did for the brute force solution, but only selecting one of the possibilities. \n",
    "\n",
    "Again, this is not the same as simply using `gen_traj`, because we are conditioning on the observation $\\vec{x}_1,\\ldots,\\vec{x}_T$. \n",
    "\n",
    "#### Task \n",
    "\n",
    "Reimplement `P_traj` where it should use your solution from last week when `M<0` and take `M` samples when `M>0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is to extract your solution from Lab 01, replace 'Lab_01_Solution.ipynb' as appropriate\n",
    "from extracter import extract_tagged_cells\n",
    "extract_tagged_cells(\"Lab_01_Solution.ipynb\",\"agent.py\",tags=[\"import numpy as np\", \"def gen_traj\", \"class Agent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent, gen_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b337d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate a trajectory\n",
    "ooo, sss = gen_traj(env,10)\n",
    "\n",
    "# Instantiate your agent\n",
    "agent = Agent(env)\n",
    "\n",
    "# Use your new implementation, by specifying M>0\n",
    "P_joint = agent.P_traj(ooo,M=50)\n",
    "\n",
    "# Create fig\n",
    "plt.figure()\n",
    "plt.bar(list(P_joint.keys()), list(P_joint.values()))\n",
    "plt.xlabel(r\"$\\mathbf{s}$\")\n",
    "plt.ylabel(r\"$p(\\mathbf{s}|\\mathbf{o})$\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23237bb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Discussion points: What happens if we only made a longer trajectory, but it was still sparse in probabilities? Would 'ancestral sampling' still be appropriate? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4faf791",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# For the remainder, let's use the first version of the enviroment\n",
    "env = Environment(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642d90a",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Machine Learning for Autonomous Agents\n",
    "\n",
    "Until now we supposed full unfettered access to the environment. But now suppose that we do *not* have access to the environment definition. Rather, we need to learn it. Suppose that we *do* have access to trajectories from the enviroment, generated by some agent interacting with it,  \n",
    "$$\n",
    "    \\mathbf{o}_1,\\ldots,\\mathbf{o}_T \\sim p\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{a} \\sim \\pi(\\mathbf{o}_1,\\ldots,\\mathbf{o}_T)\n",
    "$$\n",
    "where (in the following) $\\mathbf{a}$ is a one-hot-encoded action, relating to the number of possible actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724548bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import pytorch\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e2617",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "#### Generating data \n",
    "\n",
    "Let's generate data, as \n",
    "$$\n",
    "    D = \\{(\\mathbf{o}_{1:T}, \\mathbf{a}_i\\}\n",
    "$$\n",
    "where $\\mathbf{a}$ is one-hot-encoded.  \n",
    "\n",
    "So dataset `D` will contain $n$ tuples, each containing $T$ observations (dimensionality $2$) and $T$ actions (dimensionality `n_states`). \n",
    "\n",
    "This is as if recording an expert agent performing in the environment. Note that we could also include directly the action taken by an agent, rather assuming the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly visualise\n",
    "T=10\n",
    "ooo, sss = gen_traj(env,T)\n",
    "fig, ax = env.render(sss, ooo, title=r\"$\\{s^{(i)}_1,\\ldots,s^{(i)}_T\\}_{i=1}^n$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset\n",
    "D = []\n",
    "n_states = env.n_states\n",
    "trajs = []\n",
    "n = 1000\n",
    "T = 6\n",
    "\n",
    "for i in range(n):\n",
    "    ooo, sss = gen_traj(env,T)\n",
    "    trajs.append(sss)\n",
    "    ooo = torch.tensor(ooo, dtype = torch.float)\n",
    "    sss = torch.tensor(sss)\n",
    "    # We also have the (more realistic) option to use the expert agent's action here\n",
    "    one_hot = torch.nn.functional.one_hot(sss, num_classes=n_states)\n",
    "    D.append((ooo, one_hot.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2cef3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "#### Transformers\n",
    "\n",
    "We will be using transformers, which are excellent for sequential modelling. Have a look at the following `class`, which we will use in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ec841",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TinyTransformerDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim=16, num_layers=1, num_heads=1, max_seq_len=500):\n",
    "        super(TinyTransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_dim, embedding_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = self._generate_positional_encoding(max_seq_len, embedding_dim)\n",
    "        \n",
    "        # Use TransformerEncoder with batch_first=True\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embedding_dim * 2,\n",
    "            dropout=0.,  # small model/dataset\n",
    "            batch_first=True  # This is the key change\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_projection = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def _generate_positional_encoding(self, max_seq_len, embedding_dim):\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n",
    "        pos_encoding = torch.zeros(max_seq_len, embedding_dim)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding  # Shape: (max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        x_emb = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_encoding = self.positional_encoding[:x.size(1), :].to(x.device)\n",
    "        x_emb = x_emb + pos_encoding\n",
    "\n",
    "        # Generate causal mask\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(x.size(1))\n",
    "        \n",
    "        # Use mask in the encoder\n",
    "        x_transformed = self.transformer(x_emb, mask=causal_mask)\n",
    "        \n",
    "        output = self.output_projection(x_transformed)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe4e95",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Imitation Learning \n",
    "\n",
    "Imitation learning is an alternative to reinforcement learning, for producing an autonomous agent. Rather than learn via reinforcement, we can learn to imitate an expert agent (possibly, a human) that has already solved a given task. \n",
    "\n",
    "In this case, we would suppose that the agent which has generated the trajectories in data set `D` is such an expert. \n",
    "\n",
    "Specifically, in imitation learning (and in this example), we seek\n",
    "$$\n",
    "    \\mathbf{a}_t = \\pi(\\mathbf{o}_1, \\ldots, \\mathbf{o}_t)\n",
    "$$ \n",
    "where $\\pi$ the policy of the agent, that maps inputs to an action. \n",
    "\n",
    "In the case of the Environment studied in Lab 01, is it clear why we should consider the full history of observations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = n_states\n",
    "embedding_dim = 16\n",
    "num_layers = 1\n",
    "num_heads = 1\n",
    "seq_length = 6\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25cb51",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Task \n",
    "\n",
    "Use the `TinyTransformerDecoder` defined above to complete the Imitation Learning task in the cell below.\n",
    "\n",
    "**Important** Do not remove the first comment from the cell `# TASK 2 IMITATION LEARNING` as it will help us locate your implementation. \n",
    "\n",
    "**Important** Your configuration should complete in less than 5 minutes on a personal laptop machine (noting that 2 minutes should be more than enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a82c4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TASK 2 IMITATION LEARNING\n",
    "\n",
    "model_il = TinyTransformerDecoder(input_dim, output_dim, embedding_dim=embedding_dim, num_layers=num_layers, num_heads=num_heads)\n",
    "dataloader = torch.utils.data.DataLoader(D, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# TODO  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f6959",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "How well is our model (agent) performing (the one learned via imitation learning)?\n",
    "\n",
    "In the following, we deploy the agent and test it, in a similar way to Lab 01 (on the same environment). \n",
    "\n",
    "First, we wrap the model inference in the `act` function of an Agent class (namely, `DTAgent` below), so we can compare. \n",
    "\n",
    "#### Task\n",
    "\n",
    "Complete the `act` function in the `DTAgent` class below to return an integer indicating the action, as per the description of the scenario given already in Lab 01. \n",
    "\n",
    "Hint: to get probabilities, use `torch.softmax(outputs, axis = 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6ec95",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "class DTAgent:\n",
    "\n",
    "    def act(self,ooo):\n",
    "        '''\n",
    "        Decide on the best action to take, under the provided observation. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        ooo : array_like(int,ndim=2)\n",
    "            t observations (of 2 bits each)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        a : int\n",
    "            the chosen action a, it should be one of the n_states\n",
    "        '''\n",
    "\n",
    "        # Predict\n",
    "        input_tensor = torch.tensor(ooo, dtype=torch.float32).unsqueeze(0)  \n",
    "        # TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b722376",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "In the following, we perform the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "dt_agent = DTAgent()\n",
    "n_test = 1000\n",
    "T_test = 6\n",
    "score = 0\n",
    "\n",
    "# Generate some trajectories from the environment \n",
    "n_test=1000\n",
    "for i in range(n_test): \n",
    "    oooo, ssss = gen_traj(env,T_test)\n",
    "    oooo = torch.tensor(oooo, dtype = torch.float32)\n",
    "    a_t = dt_agent.act(oooo)\n",
    "    score += env.rwd(a_t,ssss[-1])\n",
    "score = score/n_test\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100fc2d0",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Model Learning\n",
    "\n",
    "Imitation Learning in the real world can be a bit risky. If we had a *model* of the environment, we could 'practice'. Namely, we seek\n",
    "$$\n",
    "    \\mathbf{x}_{t+1} \\sim P(\\mathbf{x}_{t+1} | \\mathbf{x}_1, \\ldots, \\mathbf{x}_t)\n",
    "$$ \n",
    "\n",
    "With this we could generate trajectories, with which to safely train an agent in simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = 2\n",
    "embedding_dim = 16\n",
    "num_layers = 1\n",
    "num_heads = 1\n",
    "seq_length = 6\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d690da",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Task \n",
    "\n",
    "Use the `TinyTransformerDecoder` to complete the Model Learning task in the cell below. Again, do not replace or remove the `TASK` tag.\n",
    "\n",
    "Then, complete the `sample_next_obs` function of the `DTEnvironment` class to sample \n",
    "\n",
    "**Important** Do not remove the first comment from the cell `# TASK 3 MODEL LEARNING` as it will help us locate your implementation. \n",
    "\n",
    "**Important** Your configuration should complete in less than 5 minutes on a personal laptop machine (noting that 2 minutes should be more than enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3 MODEL LEARNING\n",
    "model_ml = TinyTransformerDecoder(input_dim, output_dim, embedding_dim=embedding_dim, num_layers=num_layers, num_heads=num_heads)\n",
    "dataloader = torch.utils.data.DataLoader(D, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# TODO  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888ffe0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DTEnvironment:\n",
    "\n",
    "    def sample_next_obs(self,ooo):\n",
    "        '''\n",
    "        Provide a next observation, to follow the provided observation. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        ooo : array_like(ndim=2)\n",
    "            t observations (of 2 bits each)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        o : array_like(ndim=1)\n",
    "            the next observation as an array of 2 ints\n",
    "        '''\n",
    "\n",
    "        input_tensor = torch.tensor(ooo, dtype=torch.float32)\n",
    "\n",
    "        model_ml.eval()\n",
    "        with torch.no_grad():  # Disable gradient comp for inference\n",
    "            obs = model_ml(input_tensor.unsqueeze(0))\n",
    "        obs = torch.sigmoid(obs)\n",
    "\n",
    "        # Extract the next observation\n",
    "        final_obs = obs[:, -1, :]\n",
    "        \n",
    "        return torch.bernoulli(final_obs).int().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968f5aa",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "Once we have a model of the environment, we can simulate new trajectories, as exemplified as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_env = DTEnvironment()\n",
    "# Generate some trajectories from the environment \n",
    "T=10\n",
    "oooo = np.zeros((T,2))\n",
    "for t in range(1,T):\n",
    "    out = dt_env.sample_next_obs(oooo[0:t,:])\n",
    "    oooo[t,:] = dt_env.sample_next_obs(oooo[0:t,:])\n",
    "\n",
    "print(oooo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4aa25e",
   "metadata": {},
   "source": [
    "You can use your `p_traj` function to check the 'legitimacy' of these trajectories (they should have probability greater than 0). \n",
    "\n",
    "Of course, in a real-world environment, validating the trajectories is more subjective, or requires human expertise. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)\n",
    "P_joint = agent.P_traj(oooo.astype(int))\n",
    "\n",
    "# Check should sum to 1 (or close enough)\n",
    "probs = list(P_joint.values())\n",
    "assert abs(sum(probs) - 1) <= 0.05\n",
    "\n",
    "# Extract possible paths\n",
    "paths = [np.fromstring(k, sep=' ') for k in P_joint.keys()] \n",
    "\n",
    "# Take some samples\n",
    "sample_indices = np.random.choice(len(probs), size=10, p=probs)\n",
    "trajs = [paths[i].astype(int) for i in sample_indices]\n",
    "\n",
    "# Visualise \n",
    "fig, ax = env.render(trajs[0], oooo, title=r\"$s_1,\\ldots,s_T \\sim \\hat P_\\theta(\\cdot \\mid o_1,\\ldots,o_T)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbee3ff",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## A Shared Representation\n",
    "\n",
    "Notice that both the previous tasks (imitation learning and model learning) have something in common; namely the input. It suggests it may be more efficient to share a representation in a deep architecture, rather than reimplementing both as separate networks. Such architectures will also be very useful later for reinforcement learning. \n",
    "\n",
    "Have a look at the architecture defined in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformerDecoderDoubleHead(nn.Module):\n",
    "    def __init__(self, input_dim, act_dim, obs_dim, embedding_dim=16, num_layers=1, num_heads=1, max_seq_len=500):\n",
    "        super(TinyTransformerDecoderDoubleHead, self).__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_dim, embedding_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = self._generate_positional_encoding(max_seq_len, embedding_dim)\n",
    "        \n",
    "        # Use TransformerEncoder with batch_first=True\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embedding_dim * 2,\n",
    "            dropout=0.,  # small model/dataset\n",
    "            batch_first=True  # This is the key change\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.act_projection = nn.Linear(embedding_dim, act_dim)\n",
    "        self.obs_projection = nn.Linear(embedding_dim, obs_dim)\n",
    "        \n",
    "    def _generate_positional_encoding(self, max_seq_len, embedding_dim):\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n",
    "        pos_encoding = torch.zeros(max_seq_len, embedding_dim)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding  # Shape: (max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        x_emb = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_encoding = self.positional_encoding[:x.size(1), :].to(x.device)\n",
    "        x_emb = x_emb + pos_encoding\n",
    "\n",
    "        # Generate causal mask\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(x.size(1))\n",
    "        \n",
    "        # Use mask in the encoder\n",
    "        x_transformed = self.transformer(x_emb, mask=causal_mask)\n",
    "        \n",
    "        act = self.act_projection(x_transformed)\n",
    "        obs = self.obs_projection(x_transformed)\n",
    "        return act, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9626f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "act_dim = 30\n",
    "obs_dim = 2\n",
    "embedding_dim = 16\n",
    "num_layers = 1\n",
    "num_heads = 1\n",
    "seq_length = 6\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf00cb1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "#### Task \n",
    "\n",
    "Implement the Architecture in PyTorch using a double head (and shared inner representations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cad1da",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# TASK 4 SHARED REPRESENTATION \n",
    "model_shared = TinyTransformerDecoderDoubleHead(input_dim, act_dim, obs_dim, embedding_dim=embedding_dim, num_layers=num_layers, num_heads=num_heads)\n",
    "dataloader = torch.utils.data.DataLoader(D, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# TODO  "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
