\documentclass{article}
\usepackage{C:/Users/Admin-PC/Documents/git_repository/tpack/tpack}
% \usepackage{/home/tr0fin0/git_repositories/tpack/tpack}


\title{IC202 - Information et Réseaux}
\project{Résumé Théorique}
\author{Guilherme Nunes Trofino}
\authorRA{217276}


\makeatletter
\begin{document}\selectlanguage{french}
\maketitle

\newpage\tableofcontents

\section{Introduction}
\subfile{C:/Users/Admin-PC/Documents/git_repository/classes_ensta/intro.tex}
% \subfile{/home/tr0fin0/git_repositories/classes_ensta/intro.tex}


\subsection{Informations Matière}
\paragraph{Présentation}Ce cours sera présenter par M. Benoît GELLER qui a pour but d'étudier Théorie d'Information.

\subsection{Introduction à la Théorie de L'Information}

\section{Entropie}
\paragraph{Définition}

\subsection{Incertitude}
\paragraph{\href{https://fr.wikipedia.org/wiki/Incertitude}{Définition}}Imprevisbilité d'un variable sera donne par l'equation suivante:
\begin{equation}\label{eq:incertitude}
    \boxed{
        h(x) = h(X=x) = \log_{2} \left(\frac{1}{p(x)}\right)
    }
\end{equation}
Où $p(x)$ represente la probabilité que la variable aléatoire $X$ prenne la valeur $x$.

\paragraph{Postulat}Cette définition à été porposé par \href{https://fr.wikipedia.org/wiki/Claude_Shannon}{Shannon} basé sur les postulats suivantes:
\begin{enumerate}
    \item L'incertitude est inversement proportionnelle à la probabilité;
    \item L'incertitude de deux évènements indépendants est la somme des incertitudes;
\end{enumerate}
Le première postulat implique na probabilité inversement proportionnelle et le deuxième postulat implique une fonction logarithmique. La base du logarithm est arbitraire et Shannon a choisi la base 2 en nomant l'unité d'information \textbf{bit}, qui n'a rien à voir avec les éléments binaires de nos ordinateurs.


\subsection{Variable Discrète}
\paragraph{Définition}

\subsubsection{Esperance}
\paragraph{Définition}
\begin{equation}\label{eq:esperance_discrete}
    \boxed{
        \mathbb{E}[X] = \sum_{i=1}^{\infty} x_i p_i
    }
\end{equation}

\subsubsection{Entropie}
\paragraph{Définition} \href{https://www.youtube.com/watch?v=nAA7UyiCIOE}{Entropie}
\begin{equation}
    \boxed{
        H(X) = \mathbb{E}[h(x)] = 
        \sum_{i=1}^{N_x} p(x_i)\,h(x_i) = 
        \sum_{i=1}^{N_x} p(x_i)\,\log_{2}\left(\frac{1}{p(x_i)}\right)
    }
\end{equation}

\subsection{Variable Continue}
\paragraph{Définition}

\subsubsection{Esperance}
\paragraph{Définition}
\begin{equation}\label{eq:esperance_continue}
    \boxed{
        \mathbb{E}[X] = \int_{-\infty}^{+\infty} x p(x)\,dx
    }
\end{equation}

\subsubsection{Entropie Différentielle}
\paragraph{Définition}
\begin{equation}\label{eq:entropie_differentielle}
    \boxed{
        H(X) = \mathbb{E}[h(x)] = \int_{D_x} p(x) \log_{2} \left( \frac{1}{p(x)} \right) dx
    }
\end{equation}
Où on considere des variables aléatoires $X$ prennant des valeurs continue avec une certaine densité de probabilité $p(x)$ où $x$ appartient à un intervalle réel $D_x$.

\subsection{Distribuitions}
\subsubsection{Loi Normale}
\paragraph{Définition}
\begin{equation}\label{eq:loi_normale}
    \boxed{
        p(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp{\frac{-(x - \mu)^2}{2\sigma^2}}
    }
\end{equation}


\section{Exercices}

\subsection{Introduction à la Théorie de L'Information}
\subsubsection{Exercice 1}
\paragraph{Déclaration}Calculer l'entropie différentielle d'une variable aléatoire Gaussienne
\begin{resolution}
Tout d'abbord on considere l'Entropie Différentielle $H(X)$, l'equation \ref{eq:entropie_differentielle}, et la Distribuition d'une Variable Aléatoire Gaussienne, l'equation:
\begin{align*}
    H(X) &= \int_{D_x} p(x) \log_{2} \left( \frac{1}{p(x)} \right) dx
    \quad\text{où}\quad
    p(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp{\frac{-(x - \mu)^2}{2\sigma^2}}
\end{align*}
Il faut déveloper les comptes et il faut savoir que:
\begin{enumerate}
    \item Définition de Distribuition:
    \begin{equation}
        \int_{\mathbb{R}} \frac{1}{\sigma\sqrt{2\pi}} \exp{\frac{-(x - \mu)^2}{2\sigma^2}}\,dx = 1
    \end{equation}
    \item Définition de Variance:
    \begin{equation}
        \int_{\mathbb{R}} \frac{-(x - \mu)^2}{\sigma\sqrt{2\pi}} \exp{\frac{-(x - \mu)^2}{2\sigma^2}}\,dx = \sigma^2
    \end{equation}
\end{enumerate}
Après des manipulations on retrouver que:
\begin{equation}
    \boxed{
        H(x) = \log_{2} (\sqrt{2\pi e \sigma^2})
    }
\end{equation}
\end{resolution}

\subsubsection{Exercice 2}
\paragraph{Déclaration}Montrer que l'entropie différentielle d'une variable aléatoire de variance fixée $\sigma_{X}^{2}$, prenant des valeurs réelles quelconques, est maximum pour la variable Gaussienne.
\begin{resolution}
    
\end{resolution}

\subsubsection{Exercice 3}
\begin{resolution}
    
\end{resolution}

\end{document}