\documentclass{article}
% \usepackage{C:/Users/Admin-PC/Documents/git_repository/tpack/tpack}
\usepackage{/home/tr0fin0/git_repositories/tpack/tpack}


\title{IC202 - Information et Réseaux}
\project{Résumé Théorique}
\author{Guilherme Nunes Trofino}
\authorRA{217276}


\makeatletter
\begin{document}\selectlanguage{french}
\maketitle

\newpage\tableofcontents

\section{Introduction}
\subfile{/home/tr0fin0/git_repositories/classes_ensta/intro.tex}
% \subfile{C:/Users/Admin-PC/Documents/git_repository/classes_ensta/intro.tex}\paragraph{Présentation}


\subsection{Informations Matière}
\paragraph{Présentation}Ce cours sera présenter par M. Benoît GELLER qui a pour but d'étudier ... .

\subsection{Introduction à la Théorie de L'Information}

\section{Entropie}
\paragraph{Définition}

\subsection{Incertitude}

\section{Exercices}

\subsection{Introduction à la Théorie de L'Information}
\subsubsection{Exercice 1}
\paragraph{Déclaration}Calculer l'entropie différentielle d'une variable aléatoire Gaussienne
\begin{resolution}
Tout d'abbord on considere l'Entropie Différentielle $H(X)$, l'equation \ref{eq:entropie_differentielle}, et la Distribuition d'une Variable Aléatoire Gaussienne, l'equation:
\begin{align*}
    H(X) &= \int_{D_x} p(x) \log_{2} \left( \frac{1}{p(x)} \right) dx
    \quad\text{où}\quad
    p(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp{\frac{-(x - \mu)^2}{2\sigma^2}}
\end{align*}
Il faut déveloper les comptes et il faut savoir que:
\begin{enumerate}
    \item Définition de Distribuition:
    \begin{equation}
        \int_{\mathbb{R}} \frac{1}{\sigma\sqrt{2\pi}} \exp{\frac{-(x - \mu)^2}{2\sigma^2}}\,dx = 1
    \end{equation}
    \item Définition de Variance:
    \begin{equation}
        \int_{\mathbb{R}} \frac{-(x - \mu)^2}{\sigma\sqrt{2\pi}} \exp{\frac{-(x - \mu)^2}{2\sigma^2}}\,dx = \sigma^2
    \end{equation}
\end{enumerate}
Après des manipulations on retrouver que:
\begin{equation}
    \boxed{
        H(x) = \log_{2} (\sqrt{2\pi e \sigma^2})
    }
\end{equation}
\end{resolution}

\subsubsection{Exercice 2}
\paragraph{Déclaration}Montrer que l'entropie différentielle d'une variable aléatoire de variance fixée $\sigma_{X}^{2}$, prenant des valeurs réelles quelconques, est maximum pour la variable Gaussienne.
\begin{resolution}
    
\end{resolution}

\subsubsection{Exercice 3}
\begin{resolution}
    
\end{resolution}

\end{document}